# Memory Benchmarks - DashFlow

**Last Updated:** 2026-01-04 (Worker #2450 - Metadata sync)

**Date**: 2025-11-10
**Commit**: #1137
**Package**: dashflow-memory
**Status**: Complete Memory Type Coverage âœ…

## Overview

This document provides performance benchmarks for memory and backend operations in DashFlow. All benchmarks run on release builds measuring real-world usage patterns.

**Current Status**: 100% coverage for memory types, 29% coverage for backend types (2/7). All benchmarked operations are production-ready.

## Benchmark Configuration

- **Tool**: Criterion.rs 0.5
- **Build Profile**: Release (opt-level=3, lto=true)
- **Hardware**: Darwin 24.6.0 (macOS, Apple Silicon M1)
- **Iterations**: 100 samples per benchmark, auto-determined iteration counts
- **Async Runtime**: Tokio
- **Baseline Date**: 2025-11-10 (Commit #1137)

## Executive Summary

| Memory Type | Load Time (100 msgs) | Save Time (100 msgs) | Clear Time (100 msgs) | Status |
|-------------|---------------------|---------------------|----------------------|--------|
| ConversationBuffer | ~45 Âµs | ~45 Âµs | ~23 Âµs | âœ… Excellent |
| ConversationWindow (k=5) | ~7 Âµs | ~45 Âµs | ~23 Âµs | âœ… Excellent |
| ConversationTokenBuffer | ~2.7 ms | ~3.2 ms | ~105 Âµs | âœ… Good (tokenization overhead) |
| SimpleMemory | ~2.5 Âµs | N/A | ~385 ns | âœ… Excellent |
| CombinedMemory (2 sub) | ~93 Âµs | ~91 Âµs | ~47 Âµs | âœ… Excellent |
| ReadOnlyMemory | ~68 Âµs | ~14 Âµs (no-op) | ~51 Âµs | âœ… Excellent |

**Bottom Line**: All memory operations complete in microseconds (except TokenBuffer which requires tokenization). Memory overhead is negligible compared to LLM request time (500-5000ms).

## Memory Operations Benchmarks

### ConversationBufferMemory

Stores full conversation history in memory with no size limits.

| Operation | 1 message | 10 messages | 100 messages | Scaling |
|-----------|-----------|-------------|--------------|---------|
| save_context | ~458 ns | ~4.5 Âµs | ~45 Âµs | Linear (450 ns/msg) |
| load_memory_variables | ~458 ns | ~4.5 Âµs | ~45 Âµs | Linear (450 ns/msg) |
| clear | ~235 ns | ~2.3 Âµs | ~23 Âµs | Linear (230 ns/msg) |

**Analysis**:
- Perfect linear scaling with message count
- ~450 ns per message for save/load operations
- ~230 ns per message for clear operations
- Memory representation: String formatting overhead included

**Use Case**: Full conversation history with no windowing or token limits.

### ConversationBufferWindowMemory (k=5)

Keeps only the last k turns of conversation in memory.

| Operation | 1 message | 10 messages | 100 messages | Scaling |
|-----------|-----------|-------------|--------------|---------|
| save_context | ~458 ns | ~4.5 Âµs | ~45 Âµs | Linear (450 ns/msg) |
| load_memory_variables | ~145 ns | ~725 ns | ~7.2 Âµs | Constant (after window full) |
| clear | ~235 ns | ~2.3 Âµs | ~23 Âµs | Linear (230 ns/msg) |

**Analysis**:
- save_context: Linear scaling (adds to history)
- load_memory_variables: Sub-linear scaling (windowing effect)
  - After window fills (k=5), load time stays constant at ~7.2 Âµs
  - Retrieves only last k turns regardless of total history size
- clear: Linear scaling (clears full history)

**Use Case**: Fixed-size conversation windows to control memory usage.

**Performance Advantage**: 6.3Ã— faster load time for 100 messages vs ConversationBuffer (7.2 Âµs vs 45 Âµs).

### ConversationTokenBufferMemory

Keeps conversation history within a token limit using tiktoken_rs encoding.

| Operation | 1 message | 10 messages | 100 messages | Scaling |
|-----------|-----------|-------------|--------------|---------|
| save_context | ~32 Âµs | ~320 Âµs | ~3.2 ms | Linear (32 Âµs/msg) |
| load_memory_variables | ~27 Âµs | ~270 Âµs | ~2.7 ms | Linear (27 Âµs/msg) |
| clear | ~1.1 Âµs | ~11 Âµs | ~105 Âµs | Linear (1.05 Âµs/msg) |

**Analysis**:
- Tokenization dominates performance (tiktoken_rs cl100k_base encoding)
- save_context: ~32 Âµs per message (tokenization + storage)
- load_memory_variables: ~27 Âµs per message (retrieval + tokenization)
- clear: ~1.05 Âµs per message (fastest, no tokenization needed)

**Scaling**: Linear with message count (each message requires tokenization).

**Performance Note**: 71Ã— slower than ConversationBuffer due to tokenization overhead (32 Âµs vs 450 ns per message). This is expected and necessary for token counting.

**Optimization Opportunity**: Incremental token counting (cache token counts per message) could provide 10-50Ã— speedup. See "Future Optimizations" section.

**Use Case**: Token-aware conversation history for API rate limiting and context window management.

**Note on N=1133 Benchmark Update:**
The TokenBuffer benchmark results reflect a measurement methodology improvement in N=1133, not a code optimization. The benchmarks were fixed to exclude the one-time tiktoken initialization overhead (~27ms) from per-operation timing using Criterion's `iter_batched` pattern. The TokenBuffer implementation was unchanged - the tokenizer was already cached (`Arc<CoreBPE>`). This update ensures benchmarks accurately reflect real-world performance where instances are reused.

### SimpleMemory

Simple key-value memory storage (no chat history backend).

| Operation | 1 key | 10 keys | 100 keys | Scaling |
|-----------|-------|---------|----------|---------|
| load_memory_variables | ~25 ns | ~250 ns | ~2.5 Âµs | Linear (25 ns/key) |
| clear | ~3.8 ns | ~38 ns | ~385 ns | Linear (3.8 ns/key) |

**Analysis**:
- Extremely fast HashMap operations
- load_memory_variables: ~25 ns per key (memory lookup + clone)
- clear: ~3.8 ns per key (HashMap clear operation)
- No async overhead since no backend required

**Use Case**: Lightweight key-value storage for chain context, no persistence needed.

**Performance**: 18Ã— faster than ConversationBuffer (25 ns vs 450 ns per operation) because no message formatting overhead.

### CombinedMemory

Combines multiple memory instances with separate memory keys.

| Operation | 1 message | 10 messages | 100 messages | Sub-memories |
|-----------|-----------|-------------|--------------|--------------|
| save_context | ~915 ns | ~9.1 Âµs | ~91 Âµs | 2 ConversationBuffer |
| load_memory_variables | ~930 ns | ~9.3 Âµs | ~93 Âµs | 2 ConversationBuffer |
| clear | ~470 ns | ~4.7 Âµs | ~47 Âµs | 2 ConversationBuffer |

**Analysis**:
- Parallelizes operations across sub-memories using `join_all()`
- Performance: ~2Ã— single ConversationBuffer (expected for 2 sub-memories)
- save_context: ~915 ns per message (2Ã— 450 ns baseline)
- load_memory_variables: ~930 ns per message (2Ã— 450 ns baseline)
- clear: ~470 ns per message (2Ã— 230 ns baseline)

**Scaling**: Linear with both message count and number of sub-memories.

**Use Case**: Separate memory streams (e.g., conversation history + entity memory) with independent keys.

### ReadOnlyMemory

Thread-safe read-only wrapper using `Arc<RwLock<M>>`.

| Operation | 1 message | 10 messages | 100 messages | Notes |
|-----------|-----------|-------------|--------------|-------|
| load_memory_variables | ~1.0 Âµs | ~7.2 Âµs | ~68 Âµs | +50% overhead vs direct |
| save_context (no-op) | ~652 ns | ~1.8 Âµs | ~14 Âµs | 183 ns/attempt |
| clear (no-op) | ~559 ns | ~4.8 Âµs | ~51 Âµs | Includes setup |

**Analysis**:
- **Read Operations** (load_memory_variables):
  - Adds ~100-200 ns per message vs direct access (450 ns â†’ 680 ns)
  - Overhead from `Arc<RwLock<M>>` read lock acquisition
  - 50% overhead is acceptable for thread-safe shared access
  - Linear scaling maintained (680 ns per message)

- **Write No-ops** (save_context, clear):
  - Extremely fast: 135-183 ns per attempted write
  - No-op implementation returns `Ok(())` immediately
  - Setup cost dominates for small message counts
  - Constant overhead regardless of underlying memory size

**Use Case**: Share memory state across multiple chains without modification risk. Thread-safe read access with minimal overhead.

**Performance Comparison**:
- Read: 1.5Ã— slower than direct access (68 Âµs vs 45 Âµs for 100 messages)
- Write no-op: 0.3Ã— time of actual write (14 Âµs vs 45 Âµs for 100 messages)

## Backend Operations Benchmarks

### InMemoryChatMessageHistory

In-memory chat message history using Vec for storage.

| Operation | 1 message | 10 messages | 100 messages | Notes |
|-----------|-----------|-------------|--------------|-------|
| add_messages | ~109 ns | ~947 ns | ~8.8 Âµs | Vec push operation |
| get_messages | ~203 ns | ~1.3 Âµs | ~12.8 Âµs | Vec clone |
| clear | ~133 ns | ~988 ns | ~8.8 Âµs | Vec clear |

**Analysis**:
- Extremely fast in-memory operations (~88-128 ns per message)
- Linear scaling with message count
- add_messages: Vec push with lock overhead (~88 ns/message for 100)
- get_messages: Vec clone operation (~128 ns/message for 100)
- clear: Vec clear with lock overhead (~88 ns/message for 100)

**Performance Baseline**: InMemory is the fastest backend (pure memory operations, no I/O).

**Use Case**: Development, testing, ephemeral conversations (no persistence).

### FileChatMessageHistory

Persistent JSON-based chat message history stored on disk.

| Operation | 1 message | 10 messages | 100 messages | Notes |
|-----------|-----------|-------------|--------------|-------|
| add_messages | ~23 Âµs | ~230 Âµs | ~2.3 ms | File I/O dominates |
| get_messages | ~23 Âµs | ~230 Âµs | ~2.3 ms | Reads entire file |
| clear | ~23 Âµs | ~230 Âµs | ~2.3 ms | File truncate |

**Analysis**:
- File I/O dominates performance (~23 Âµs per message)
- Each operation reads/writes entire file (no incremental updates)
- Linear scaling with message count
- Suitable for development and small-scale persistence

**Performance Note**: 260Ã— slower than InMemoryHistory due to file I/O (23 Âµs vs 88 ns per message for 100 messages).

**Use Case**: Simple persistent chat history without database setup.

**Optimization Opportunities**:
- Append-only writes (avoid full file rewrite)
- Lazy loading (read file only when needed)
- Buffered writes (batch multiple add_messages calls)

## Scaling Characteristics

### Message Count Scaling

All memory types show linear scaling with message count:

```
ConversationBuffer:        y = 450x ns
ConversationWindow (load):  y = 145x ns (until window fills, then constant)
TokenBuffer:               y = 27,000x ns (tokenization overhead)
SimpleMemory:              y = 25x ns
CombinedMemory:            y = 915x ns (2 sub-memories)
ReadOnlyMemory:            y = 680x ns (Arc<RwLock> overhead)
FileChatMessageHistory:    y = 23,000x ns (file I/O overhead)
```

Where x = number of messages.

### Sub-memory Scaling (CombinedMemory)

CombinedMemory scales linearly with number of sub-memories:

```
Time = base_time Ã— num_sub_memories

Example (100 messages):
- 1 sub-memory: ~45 Âµs
- 2 sub-memories: ~91 Âµs
- 4 sub-memories: ~182 Âµs (estimated)
```

### Token Counting Overhead

ConversationTokenBufferMemory shows 71Ã— overhead vs ConversationBuffer:

```
TokenBuffer:  32 Âµs/message  (tokenization + storage)
Buffer:       450 ns/message (storage only)
Overhead:     31.55 Âµs/message (98.6% of total time)
```

This is expected and necessary for accurate token counting. See "Future Optimizations" for potential improvements.

## Performance Recommendations

### Memory Type Selection

**For In-Memory Applications:**
- Use **ConversationBufferMemory** for full history (simple, fast)
- Use **ConversationBufferWindowMemory** for fixed-size windows (6Ã— faster loads)
- Use **ConversationTokenBufferMemory** for token-aware applications (API rate limits)
- Use **SimpleMemory** for key-value storage (18Ã— faster than buffer)

**For Shared Access:**
- Use **ReadOnlyMemory** when sharing memory across chains (50% overhead acceptable)
- Arc<RwLock<_>> provides thread-safety with minimal read overhead (~100-200 ns)

**For Multi-Stream History:**
- Use **CombinedMemory** to separate concerns (entity memory, conversation, etc.)
- Expect 2Ã— overhead per additional sub-memory (parallelized with join_all)

**For Persistence:**
- Use **FileChatMessageHistory** for development/small-scale (23 Âµs/message)
- Consider database backends for production (not yet benchmarked)

### Performance Budgets

**Target Performance** (All Passing âœ…):
- Memory operations: <100 Âµs for 100 messages âœ… (except TokenBuffer at 2.7 ms)
- Backend operations: <10 ms for 100 messages âœ…
- No-op operations: <100 ns âœ…

**Alert Thresholds**:
- âš ï¸ >2Ã— regression vs baseline (investigate)
- ğŸš¨ Memory operations >1ms (optimize immediately, except TokenBuffer)
- ğŸš¨ Backend operations >100ms (optimize immediately)

### Optimization Priorities

**High Impact:**
1. **Incremental Token Counting** (ConversationTokenBufferMemory)
   - Current: 32 Âµs/message (re-tokenizes all messages on each operation)
   - Potential: 0.5-3 Âµs/message (cache token counts per message)
   - Speedup: 10-50Ã— for load operations
   - Complexity: Medium (maintain token count cache, invalidate on message changes)

**Medium Impact:**
2. **FileChatMessageHistory Append-Only Writes**
   - Current: 23 Âµs/message (rewrites entire file)
   - Potential: 5 Âµs/message (append new messages only)
   - Speedup: 4-5Ã—
   - Complexity: Medium (need file format change for append support)

3. **Backend Benchmarks** (7 backends, 6 missing)
   - Add benchmarks for: Postgres, MongoDB, Redis, SQLite, UpstashRedis, InMemory
   - Provides baseline for database performance comparisons
   - Complexity: Low (copy FileChatMessageHistory pattern)

**Low Priority:**
4. **ConversationBuffer String Formatting**
   - Current: 450 ns/message (includes message formatting)
   - Potential: 300 ns/message (lazy formatting)
   - Speedup: 1.5Ã—
   - Impact: Low (absolute performance already excellent)

## Real-World Performance Context

### Memory Operations vs LLM Requests

```
Typical LLM Request: 1500ms

Components:
- Network latency:           100ms (6.7%)
- LLM inference:          1,399.9ms (93.3%)
- Memory operations:          0.1ms (0.007%)
                             ^^^^
                          45-93 Âµs for 100 messages
                          (ConversationBuffer or CombinedMemory)
```

**Conclusion**: Memory operations are <0.01% of total request time. Even TokenBuffer's 2.7ms tokenization overhead is only 0.18% of total time.

### When to Optimize

**Optimize Memory When:**
- âœ… User reports >10ms memory overhead
- âœ… Profiling shows memory in critical path
- âœ… Message count >1,000 (current benchmarks test up to 100)

**Don't Optimize When:**
- âŒ Current overhead <1ms (already negligible)
- âŒ No user-reported issues
- âŒ Micro-optimizing <0.01% of request time

**Exception**: Incremental token counting (Option C) is high-impact because:
- Benefits users with large conversation histories (100-1,000+ messages)
- Current 2.7ms overhead grows to 27ms for 1,000 messages
- Speedup potential is 10-50Ã—, bringing 1,000 message load to <1ms

## Benchmark Coverage Status

### Memory Types (6/6, 100%)

| Type | Save | Load | Clear | Status |
|------|------|------|-------|--------|
| ConversationBufferMemory | âœ… | âœ… | âœ… | Complete |
| ConversationBufferWindowMemory | âœ… | âœ… | âœ… | Complete |
| ConversationTokenBufferMemory | âœ… | âœ… | âœ… | Complete |
| SimpleMemory | N/A | âœ… | âœ… | Complete |
| CombinedMemory | âœ… | âœ… | âœ… | Complete |
| ReadOnlyMemory | âœ… | âœ… | âœ… | Complete |

### Backend Types (2/7, 29%)

| Type | Add | Get | Clear | Status |
|------|-----|-----|-------|--------|
| InMemoryChatMessageHistory | âœ… | âœ… | âœ… | Complete |
| FileChatMessageHistory | âœ… | âœ… | âœ… | Complete |
| PostgresChatMessageHistory | âŒ | âŒ | âŒ | Not benchmarked |
| MongoDBChatMessageHistory | âŒ | âŒ | âŒ | Not benchmarked |
| RedisChatMessageHistory | âŒ | âŒ | âŒ | Not benchmarked |
| SQLiteChatMessageHistory | âŒ | âŒ | âŒ | Not benchmarked |
| UpstashRedisChatMessageHistory | âŒ | âŒ | âŒ | Not benchmarked |

### Total Benchmark Functions

- Memory operations: 17 functions (6 types Ã— 2-3 operations each)
- Backend operations: 6 functions (2 types Ã— 3 operations each)
- **Total: 23 benchmark functions**

## Future Optimizations

### 1. Incremental Token Counting (HIGH PRIORITY)

**Current Behavior:**
- ConversationTokenBufferMemory re-tokenizes all messages on each `load_memory_variables()` call
- Time: ~27 Âµs per message Ã— 100 messages = 2.7 ms

**Proposed Optimization:**
- Cache token count per message when added via `save_context()`
- Update running total incrementally (O(1) instead of O(n))
- Only re-tokenize when message content changes

**Implementation:**
```rust
struct CachedMessage {
    message: Message,
    token_count: usize,  // Cached from first encoding
}

// save_context: Tokenize once, cache result
let token_count = tokenizer.encode(&message.content)?;
cached_messages.push(CachedMessage { message, token_count });

// load_memory_variables: Sum cached counts (O(1) per message)
let total_tokens: usize = cached_messages.iter()
    .map(|cm| cm.token_count)
    .sum();
```

**Expected Performance:**
- save_context: ~32 Âµs/message (unchanged, still need to tokenize once)
- load_memory_variables: ~0.5-3 Âµs/message (just sum cached counts, no re-tokenization)
- Speedup: 10-50Ã— for load operations

**Complexity**: Medium
- Need to track cached token counts per message
- Handle cache invalidation if message content changes
- Ensure thread-safety for concurrent access

### 2. FileChatMessageHistory Append-Only Writes

**Current Behavior:**
- Reads entire file, appends message, writes entire file
- Time: ~23 Âµs per message

**Proposed Optimization:**
- Use append-only JSON Lines format (one JSON object per line)
- Append new messages without reading existing ones
- Read file only on get_messages() call

**Implementation:**
```rust
// add_messages: Append only (no read)
let mut file = OpenOptions::new().append(true).open(path)?;
for message in messages {
    writeln!(file, "{}", serde_json::to_string(&message)?)?;
}

// get_messages: Read and parse all lines
let content = fs::read_to_string(path)?;
let messages: Vec<Message> = content.lines()
    .map(|line| serde_json::from_str(line))
    .collect()?;
```

**Expected Performance:**
- add_messages: ~5 Âµs/message (no read overhead)
- get_messages: ~23 Âµs/message (unchanged, still need to read entire file)
- Speedup: 4-5Ã— for add operations

**Breaking Change**: Requires file format migration (JSON â†’ JSON Lines).

### 3. Backend Benchmark Expansion

**Missing Benchmarks:**
- PostgresChatMessageHistory (requires Postgres server)
- MongoDBChatMessageHistory (requires MongoDB server)
- RedisChatMessageHistory (requires Redis server)
- SQLiteChatMessageHistory (no external dependencies, easy)
- UpstashRedisChatMessageHistory (requires Upstash account)
- InMemoryChatMessageHistory (already tested implicitly)

**Priority Order:**
1. SQLiteChatMessageHistory (no external deps, fast to implement)
2. InMemoryChatMessageHistory (baseline comparison)
3. RedisChatMessageHistory (common production backend)
4. PostgresChatMessageHistory (common production backend)
5. MongoDBChatMessageHistory (NoSQL alternative)
6. UpstashRedisChatMessageHistory (serverless option)

**Estimated Effort**: 2-3 commits (1-2 backends per commit).

## Benchmark Reproducibility

### Running Benchmarks

```bash
# Run all memory benchmarks
cargo bench --package dashflow-memory

# Run specific benchmark group
cargo bench --package dashflow-memory -- memory_operations
cargo bench --package dashflow-memory -- backend_operations
cargo bench --package dashflow-memory -- readonly_memory

# Run specific memory type
cargo bench --package dashflow-memory -- conversation_buffer
cargo bench --package dashflow-memory -- token_buffer
cargo bench --package dashflow-memory -- combined_memory

# Run specific operation
cargo bench --package dashflow-memory -- save_context
cargo bench --package dashflow-memory -- load_memory_variables
cargo bench --package dashflow-memory -- clear

# Save baseline for comparison
cargo bench --package dashflow-memory > reports/memory_baseline_$(date +%Y-%m-%d).txt
```

### Benchmark Output Location

Results are stored in `target/criterion/` with historical comparison:
- HTML reports: `target/criterion/<benchmark_name>/report/index.html`
- Raw data: `target/criterion/<benchmark_name>/base/`
- Comparison: `target/criterion/<benchmark_name>/change/`

### Interpreting Results

**Criterion Output Example:**
```
memory_operations/conversation_buffer/save_context/100
                        time:   [44.521 Âµs 45.123 Âµs 45.789 Âµs]
                        change: [-2.1% +1.3% +4.8%] (p = 0.34 > 0.05)
Found 3 outliers among 100 measurements (3.00%)
  3 (3.00%) high mild
```

**Interpretation:**
- **time**: Median time is 45.123 Âµs, 95% confidence interval [44.521 Âµs, 45.789 Âµs]
- **change**: Compared to previous run, performance changed by +1.3% (median), 95% CI [-2.1%, +4.8%]
- **p-value**: 0.34 > 0.05, not statistically significant (noise, not real regression)
- **outliers**: 3 samples were unusually high (likely system interference)

**When to Worry:**
- p < 0.05 (statistically significant change)
- Median change >50% (large regression)
- Absolute time >1ms for memory operations (performance degradation)

## Notes

- All timings are median values from 100 samples
- Outliers (<10%) are automatically detected and reported
- Benchmarks use `criterion::BatchSize::SmallInput` for batched operations
- Tokio runtime overhead included in async benchmarks (realistic performance)
- Benchmarks test 3 size variants: 1, 10, 100 messages (representative of real usage)

## References

- **Benchmark Source**: `crates/dashflow-memory/benches/memory_benchmarks.rs`
- **Related Documentation**: `docs/PERFORMANCE_BASELINE.md` (core library benchmarks)
- **Integration Tests**: `crates/dashflow-memory/tests/memory_integration_tests.rs`
