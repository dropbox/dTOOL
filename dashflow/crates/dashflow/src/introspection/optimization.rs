// Allow clippy warnings for optimization suggestions
// - clone_on_ref_ptr: Arc<Mutex> cloned for shared analysis state
// - unwrap_used: unwrap() on analysis results with validated inputs
#![allow(clippy::clone_on_ref_ptr, clippy::unwrap_used)]

//! Optimization Suggestions
//!
//! This module provides types for generating and managing optimization suggestions
//! based on execution analysis.

use super::bottleneck::BottleneckMetric;
use super::trace::{ExecutionTrace, NodeExecution};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

// Optimization Suggestions
// ============================================================================

/// Category of optimization suggestion
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub enum OptimizationCategory {
    /// Add caching to avoid redundant computation
    Caching,
    /// Run nodes in parallel instead of sequentially
    Parallelization,
    /// Use a different model (faster or more efficient)
    ModelChoice,
    /// Reduce token usage through prompt optimization
    TokenOptimization,
    /// Add error handling or retries
    ErrorHandling,
    /// Reduce execution frequency
    FrequencyReduction,
    /// Reduce variability in execution time
    Stabilization,
    /// General performance improvement
    Performance,
}

impl std::fmt::Display for OptimizationCategory {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Caching => write!(f, "caching"),
            Self::Parallelization => write!(f, "parallelization"),
            Self::ModelChoice => write!(f, "model_choice"),
            Self::TokenOptimization => write!(f, "token_optimization"),
            Self::ErrorHandling => write!(f, "error_handling"),
            Self::FrequencyReduction => write!(f, "frequency_reduction"),
            Self::Stabilization => write!(f, "stabilization"),
            Self::Performance => write!(f, "performance"),
        }
    }
}

/// Priority level for optimization suggestions
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize, Default)]
pub enum OptimizationPriority {
    /// Low priority - nice to have
    #[default]
    Low,
    /// Medium priority - should consider
    Medium,
    /// High priority - recommended
    High,
    /// Critical priority - strongly recommended
    Critical,
}

impl std::fmt::Display for OptimizationPriority {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Low => write!(f, "low"),
            Self::Medium => write!(f, "medium"),
            Self::High => write!(f, "high"),
            Self::Critical => write!(f, "critical"),
        }
    }
}

/// An optimization suggestion based on execution trace analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationSuggestion {
    /// Category of optimization
    pub category: OptimizationCategory,
    /// Target node(s) for the optimization
    pub target_nodes: Vec<String>,
    /// Human-readable description of the suggestion
    pub description: String,
    /// Expected improvement from applying the optimization
    pub expected_improvement: String,
    /// Implementation guidance
    pub implementation: String,
    /// Priority level
    pub priority: OptimizationPriority,
    /// Estimated effort (1-5 scale, 1 being easiest)
    pub effort: u8,
    /// Confidence level (0.0-1.0) in this suggestion
    pub confidence: f64,
    /// Related bottleneck that triggered this suggestion (if any)
    pub related_bottleneck: Option<BottleneckMetric>,
    /// Additional context or evidence
    pub evidence: Vec<String>,
}

impl OptimizationSuggestion {
    /// Create a new optimization suggestion
    #[must_use]
    pub fn new(
        category: OptimizationCategory,
        target_nodes: Vec<String>,
        description: impl Into<String>,
        expected_improvement: impl Into<String>,
        implementation: impl Into<String>,
    ) -> Self {
        Self {
            category,
            target_nodes,
            description: description.into(),
            expected_improvement: expected_improvement.into(),
            implementation: implementation.into(),
            priority: OptimizationPriority::Medium,
            effort: 3,
            confidence: 0.5,
            related_bottleneck: None,
            evidence: Vec::new(),
        }
    }

    /// Create a builder for optimization suggestions
    #[must_use]
    pub fn builder() -> OptimizationSuggestionBuilder {
        OptimizationSuggestionBuilder::new()
    }

    /// Set the priority
    #[must_use]
    pub fn with_priority(mut self, priority: OptimizationPriority) -> Self {
        self.priority = priority;
        self
    }

    /// Set the effort level
    #[must_use]
    pub fn with_effort(mut self, effort: u8) -> Self {
        self.effort = effort.clamp(1, 5);
        self
    }

    /// Set the confidence level
    #[must_use]
    pub fn with_confidence(mut self, confidence: f64) -> Self {
        self.confidence = confidence.clamp(0.0, 1.0);
        self
    }

    /// Set the related bottleneck
    #[must_use]
    pub fn with_related_bottleneck(mut self, metric: BottleneckMetric) -> Self {
        self.related_bottleneck = Some(metric);
        self
    }

    /// Add evidence
    #[must_use]
    pub fn with_evidence(mut self, evidence: impl Into<String>) -> Self {
        self.evidence.push(evidence.into());
        self
    }

    /// Check if this is a high priority suggestion
    #[must_use]
    pub fn is_high_priority(&self) -> bool {
        self.priority >= OptimizationPriority::High
    }

    /// Check if this is a low effort suggestion
    #[must_use]
    pub fn is_low_effort(&self) -> bool {
        self.effort <= 2
    }

    /// Get a quick win score (high priority + low effort = good quick win)
    #[must_use]
    pub fn quick_win_score(&self) -> f64 {
        let priority_score = match self.priority {
            OptimizationPriority::Low => 1.0,
            OptimizationPriority::Medium => 2.0,
            OptimizationPriority::High => 3.0,
            OptimizationPriority::Critical => 4.0,
        };
        let effort_score = 6.0 - self.effort as f64; // Invert: lower effort = higher score
        (priority_score * effort_score * self.confidence) / 4.0
    }

    /// Get a formatted summary
    #[must_use]
    pub fn summary(&self) -> String {
        format!(
            "[{}] {} ({}): {} - {}",
            self.priority,
            self.category,
            self.target_nodes.join(", "),
            self.description,
            self.expected_improvement
        )
    }

    /// Convert to JSON
    ///
    /// # Errors
    ///
    /// Returns error if serialization fails
    pub fn to_json(&self) -> Result<String, serde_json::Error> {
        serde_json::to_string_pretty(self)
    }

    /// Parse from JSON
    ///
    /// # Errors
    ///
    /// Returns error if deserialization fails
    pub fn from_json(json: &str) -> Result<Self, serde_json::Error> {
        serde_json::from_str(json)
    }
}

/// Builder for OptimizationSuggestion
#[derive(Debug, Default)]
pub struct OptimizationSuggestionBuilder {
    category: Option<OptimizationCategory>,
    target_nodes: Vec<String>,
    description: Option<String>,
    expected_improvement: Option<String>,
    implementation: Option<String>,
    priority: OptimizationPriority,
    effort: u8,
    confidence: f64,
    related_bottleneck: Option<BottleneckMetric>,
    evidence: Vec<String>,
}

impl OptimizationSuggestionBuilder {
    /// Create a new builder
    #[must_use]
    pub fn new() -> Self {
        Self {
            effort: 3,
            confidence: 0.5,
            ..Self::default()
        }
    }

    /// Set the category
    #[must_use]
    pub fn category(mut self, category: OptimizationCategory) -> Self {
        self.category = Some(category);
        self
    }

    /// Add a target node
    #[must_use]
    pub fn target_node(mut self, node: impl Into<String>) -> Self {
        self.target_nodes.push(node.into());
        self
    }

    /// Set target nodes
    #[must_use]
    pub fn target_nodes(mut self, nodes: Vec<String>) -> Self {
        self.target_nodes = nodes;
        self
    }

    /// Set the description
    #[must_use]
    pub fn description(mut self, description: impl Into<String>) -> Self {
        self.description = Some(description.into());
        self
    }

    /// Set the expected improvement
    #[must_use]
    pub fn expected_improvement(mut self, improvement: impl Into<String>) -> Self {
        self.expected_improvement = Some(improvement.into());
        self
    }

    /// Set the implementation guidance
    #[must_use]
    pub fn implementation(mut self, implementation: impl Into<String>) -> Self {
        self.implementation = Some(implementation.into());
        self
    }

    /// Set the priority
    #[must_use]
    pub fn priority(mut self, priority: OptimizationPriority) -> Self {
        self.priority = priority;
        self
    }

    /// Set the effort level
    #[must_use]
    pub fn effort(mut self, effort: u8) -> Self {
        self.effort = effort.clamp(1, 5);
        self
    }

    /// Set the confidence level
    #[must_use]
    pub fn confidence(mut self, confidence: f64) -> Self {
        self.confidence = confidence.clamp(0.0, 1.0);
        self
    }

    /// Set the related bottleneck
    #[must_use]
    pub fn related_bottleneck(mut self, metric: BottleneckMetric) -> Self {
        self.related_bottleneck = Some(metric);
        self
    }

    /// Add evidence
    #[must_use]
    pub fn evidence(mut self, evidence: impl Into<String>) -> Self {
        self.evidence.push(evidence.into());
        self
    }

    /// Build the suggestion
    ///
    /// # Errors
    ///
    /// Returns error if required fields are missing
    pub fn build(self) -> Result<OptimizationSuggestion, &'static str> {
        let category = self.category.ok_or("category is required")?;
        let description = self.description.ok_or("description is required")?;
        let expected_improvement = self
            .expected_improvement
            .ok_or("expected_improvement is required")?;
        let implementation = self.implementation.ok_or("implementation is required")?;

        if self.target_nodes.is_empty() {
            return Err("at least one target node is required");
        }

        Ok(OptimizationSuggestion {
            category,
            target_nodes: self.target_nodes,
            description,
            expected_improvement,
            implementation,
            priority: self.priority,
            effort: self.effort,
            confidence: self.confidence,
            related_bottleneck: self.related_bottleneck,
            evidence: self.evidence,
        })
    }
}

/// Analysis result containing optimization suggestions
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationAnalysis {
    /// All detected optimization suggestions
    pub suggestions: Vec<OptimizationSuggestion>,
    /// Number of patterns analyzed
    pub patterns_analyzed: usize,
    /// Overall health score (0.0-1.0, higher is better)
    pub health_score: f64,
    /// Summary of findings
    pub summary: String,
}

impl OptimizationAnalysis {
    /// Create a new optimization analysis
    #[must_use]
    pub fn new() -> Self {
        Self {
            suggestions: Vec::new(),
            patterns_analyzed: 0,
            health_score: 1.0,
            summary: String::new(),
        }
    }

    /// Check if there are any suggestions
    #[must_use]
    pub fn has_suggestions(&self) -> bool {
        !self.suggestions.is_empty()
    }

    /// Get the number of suggestions
    #[must_use]
    pub fn suggestion_count(&self) -> usize {
        self.suggestions.len()
    }

    /// Get suggestions by category
    #[must_use]
    pub fn by_category(&self, category: &OptimizationCategory) -> Vec<&OptimizationSuggestion> {
        self.suggestions
            .iter()
            .filter(|s| &s.category == category)
            .collect()
    }

    /// Get suggestions by priority
    #[must_use]
    pub fn by_priority(&self, priority: OptimizationPriority) -> Vec<&OptimizationSuggestion> {
        self.suggestions
            .iter()
            .filter(|s| s.priority == priority)
            .collect()
    }

    /// Get high priority suggestions
    #[must_use]
    pub fn high_priority(&self) -> Vec<&OptimizationSuggestion> {
        self.suggestions
            .iter()
            .filter(|s| s.is_high_priority())
            .collect()
    }

    /// Get suggestions for a specific node
    #[must_use]
    pub fn for_node(&self, node: &str) -> Vec<&OptimizationSuggestion> {
        self.suggestions
            .iter()
            .filter(|s| s.target_nodes.iter().any(|n| n == node))
            .collect()
    }

    /// Get quick win suggestions (high priority, low effort)
    #[must_use]
    pub fn quick_wins(&self) -> Vec<&OptimizationSuggestion> {
        let mut suggestions: Vec<_> = self.suggestions.iter().collect();
        suggestions.sort_by(|a, b| {
            b.quick_win_score()
                .partial_cmp(&a.quick_win_score())
                .unwrap_or(std::cmp::Ordering::Equal)
        });
        suggestions.into_iter().take(5).collect()
    }

    /// Convert to JSON
    ///
    /// # Errors
    ///
    /// Returns error if serialization fails
    pub fn to_json(&self) -> Result<String, serde_json::Error> {
        serde_json::to_string_pretty(self)
    }

    /// Parse from JSON
    ///
    /// # Errors
    ///
    /// Returns error if deserialization fails
    pub fn from_json(json: &str) -> Result<Self, serde_json::Error> {
        serde_json::from_str(json)
    }

    /// Generate a summary of the analysis
    fn generate_summary(&self) -> String {
        if self.suggestions.is_empty() {
            return "No optimization suggestions - execution looks efficient!".to_string();
        }

        let high = self.by_priority(OptimizationPriority::High).len()
            + self.by_priority(OptimizationPriority::Critical).len();
        let medium = self.by_priority(OptimizationPriority::Medium).len();
        let low = self.by_priority(OptimizationPriority::Low).len();

        let mut parts = Vec::new();
        if high > 0 {
            parts.push(format!("{} high priority", high));
        }
        if medium > 0 {
            parts.push(format!("{} medium priority", medium));
        }
        if low > 0 {
            parts.push(format!("{} low priority", low));
        }

        format!(
            "Found {} optimization suggestions: {}. Health score: {:.0}%",
            self.suggestions.len(),
            parts.join(", "),
            self.health_score * 100.0
        )
    }
}

impl Default for OptimizationAnalysis {
    fn default() -> Self {
        Self::new()
    }
}

// Add suggest_optimizations methods to ExecutionTrace
impl ExecutionTrace {
    /// Analyze execution trace and suggest optimizations
    #[must_use]
    pub fn suggest_optimizations(&self) -> OptimizationAnalysis {
        let mut analysis = OptimizationAnalysis::new();
        let mut health_deductions = 0.0;

        // Pattern 1: Detect caching opportunities (same node called multiple times)
        self.detect_caching_opportunities(&mut analysis, &mut health_deductions);

        // Pattern 2: Detect parallelization opportunities (sequential independent nodes)
        self.detect_parallelization_opportunities(&mut analysis, &mut health_deductions);

        // Pattern 3: Detect model choice optimizations (expensive models for simple tasks)
        self.detect_model_choice_opportunities(&mut analysis, &mut health_deductions);

        // Pattern 4: Detect token optimization opportunities
        self.detect_token_optimization_opportunities(&mut analysis, &mut health_deductions);

        // Pattern 5: Detect error handling improvements
        self.detect_error_handling_opportunities(&mut analysis, &mut health_deductions);

        // Pattern 6: Detect frequency reduction opportunities
        self.detect_frequency_reduction_opportunities(&mut analysis, &mut health_deductions);

        // Pattern 7: Detect stabilization opportunities
        self.detect_stabilization_opportunities(&mut analysis, &mut health_deductions);

        analysis.patterns_analyzed = 7;
        analysis.health_score = (1.0 - health_deductions).max(0.0);
        analysis.summary = analysis.generate_summary();

        // Sort suggestions by priority (highest first)
        analysis
            .suggestions
            .sort_by(|a, b| b.priority.cmp(&a.priority));

        analysis
    }

    fn detect_caching_opportunities(
        &self,
        analysis: &mut OptimizationAnalysis,
        health_deductions: &mut f64,
    ) {
        // Find nodes executed multiple times
        let mut node_counts: HashMap<&str, Vec<&NodeExecution>> = HashMap::new();
        for exec in &self.nodes_executed {
            node_counts.entry(&exec.node).or_default().push(exec);
        }

        for (node, executions) in &node_counts {
            if executions.len() >= 3 {
                // Check if executions have similar patterns (potential cache hits)
                let total_time: u64 = executions.iter().map(|e| e.duration_ms).sum();
                let avg_time = total_time as f64 / executions.len() as f64;

                // If executions are relatively consistent, caching might help
                let variance: f64 = executions
                    .iter()
                    .map(|e| (e.duration_ms as f64 - avg_time).powi(2))
                    .sum::<f64>()
                    / executions.len() as f64;
                let cv = if avg_time > 0.0 {
                    variance.sqrt() / avg_time
                } else {
                    0.0
                };

                // Low variance suggests deterministic behavior = good caching candidate
                if cv < 0.5 {
                    let priority = if executions.len() >= 10 {
                        *health_deductions += 0.1;
                        OptimizationPriority::High
                    } else if executions.len() >= 5 {
                        *health_deductions += 0.05;
                        OptimizationPriority::Medium
                    } else {
                        OptimizationPriority::Low
                    };

                    let potential_savings = total_time as f64 * 0.7; // Assume 70% cache hit rate

                    analysis.suggestions.push(
                        OptimizationSuggestion::new(
                            OptimizationCategory::Caching,
                            vec![node.to_string()],
                            format!(
                                "Node '{}' executed {} times with consistent behavior",
                                node,
                                executions.len()
                            ),
                            format!(
                                "Potential time savings: ~{:.0} ms ({:.1}% reduction)",
                                potential_savings,
                                (potential_savings / total_time as f64) * 100.0
                            ),
                            format!(
                                "Add a caching layer before '{}' to store and reuse results. Consider using a TTL-based cache if results may become stale.",
                                node
                            ),
                        )
                        .with_priority(priority)
                        .with_effort(2)
                        .with_confidence(0.7)
                        .with_related_bottleneck(BottleneckMetric::HighFrequency)
                        .with_evidence(format!(
                            "Executed {} times, avg duration: {:.0} ms, coefficient of variation: {:.2}",
                            executions.len(),
                            avg_time,
                            cv
                        )),
                    );
                }
            }
        }
    }

    fn detect_parallelization_opportunities(
        &self,
        analysis: &mut OptimizationAnalysis,
        health_deductions: &mut f64,
    ) {
        // Look for sequences of nodes that could potentially run in parallel
        // This is a heuristic - true parallelization needs graph structure info
        if self.nodes_executed.len() < 3 {
            return;
        }

        // Find groups of consecutive nodes with similar execution times
        // that might be independent
        let mut sequential_groups: Vec<Vec<&NodeExecution>> = Vec::new();
        let mut current_group: Vec<&NodeExecution> = Vec::new();

        for exec in &self.nodes_executed {
            if current_group.is_empty() {
                current_group.push(exec);
            } else {
                // SAFETY: else branch only entered when current_group is non-empty (checked above)
                #[allow(clippy::expect_used)]
                let last = current_group
                    .last()
                    .expect("current_group non-empty in else branch");
                // If similar duration and different node, might be parallelizable
                let duration_ratio = exec.duration_ms as f64 / last.duration_ms.max(1) as f64;
                if duration_ratio > 0.3 && duration_ratio < 3.0 && exec.node != last.node {
                    current_group.push(exec);
                } else {
                    if current_group.len() >= 2 {
                        sequential_groups.push(current_group);
                    }
                    current_group = vec![exec];
                }
            }
        }
        if current_group.len() >= 2 {
            sequential_groups.push(current_group);
        }

        // Suggest parallelization for groups of 3+ sequential independent-looking nodes
        for group in sequential_groups {
            if group.len() >= 3 {
                let nodes: Vec<String> = group.iter().map(|e| e.node.clone()).collect();
                let unique_nodes: std::collections::HashSet<_> = nodes.iter().cloned().collect();

                if unique_nodes.len() >= 3 {
                    let total_time: u64 = group.iter().map(|e| e.duration_ms).sum();
                    let max_time: u64 = group.iter().map(|e| e.duration_ms).max().unwrap_or(0);
                    let potential_savings = total_time.saturating_sub(max_time);

                    if potential_savings > 100 {
                        // At least 100ms savings to be worth it
                        *health_deductions += 0.05;

                        analysis.suggestions.push(
                            OptimizationSuggestion::new(
                                OptimizationCategory::Parallelization,
                                unique_nodes.into_iter().collect(),
                                format!(
                                    "{} nodes appear to run sequentially but might be independent",
                                    nodes.len()
                                ),
                                format!(
                                    "Potential time savings: ~{} ms if run in parallel",
                                    potential_savings
                                ),
                                "Consider using parallel edges or async execution if these nodes don't depend on each other's output. Review node dependencies before parallelizing.".to_string(),
                            )
                            .with_priority(OptimizationPriority::Medium)
                            .with_effort(3)
                            .with_confidence(0.5)
                            .with_evidence(format!(
                                "Sequential nodes: {}. Total time: {} ms, max single node: {} ms",
                                nodes.join(" â†’ "),
                                total_time,
                                max_time
                            )),
                        );
                    }
                }
            }
        }
    }

    fn detect_model_choice_opportunities(
        &self,
        analysis: &mut OptimizationAnalysis,
        health_deductions: &mut f64,
    ) {
        // Look for nodes with high token usage but low output complexity
        // This suggests an expensive model might be used for simple tasks
        for exec in &self.nodes_executed {
            // High tokens but fast execution might indicate over-provisioned model
            if exec.tokens_used > 1000 && exec.duration_ms < 500 {
                // Fast execution with many tokens = simple task, expensive model
                *health_deductions += 0.03;

                analysis.suggestions.push(
                    OptimizationSuggestion::new(
                        OptimizationCategory::ModelChoice,
                        vec![exec.node.clone()],
                        format!(
                            "Node '{}' uses {} tokens but executes quickly ({} ms)",
                            exec.node, exec.tokens_used, exec.duration_ms
                        ),
                        "Consider using a faster/cheaper model for this operation".to_string(),
                        format!(
                            "If '{}' performs simple tasks, consider using a smaller model (e.g., GPT-3.5-turbo instead of GPT-4) to reduce costs and latency.",
                            exec.node
                        ),
                    )
                    .with_priority(OptimizationPriority::Low)
                    .with_effort(2)
                    .with_confidence(0.4)
                    .with_related_bottleneck(BottleneckMetric::TokenUsage)
                    .with_evidence(format!(
                        "Tokens: {}, Duration: {} ms, Ratio: {:.2} tokens/ms",
                        exec.tokens_used,
                        exec.duration_ms,
                        exec.tokens_used as f64 / exec.duration_ms.max(1) as f64
                    )),
                );
            }
        }
    }

    fn detect_token_optimization_opportunities(
        &self,
        analysis: &mut OptimizationAnalysis,
        health_deductions: &mut f64,
    ) {
        // Find nodes with disproportionately high token usage
        if self.total_tokens == 0 {
            return;
        }

        let avg_tokens = self.total_tokens as f64 / self.nodes_executed.len().max(1) as f64;

        for exec in &self.nodes_executed {
            let token_percentage = (exec.tokens_used as f64 / self.total_tokens as f64) * 100.0;

            // If a single execution uses more than 50% of total tokens
            if token_percentage > 50.0 {
                *health_deductions += 0.08;

                analysis.suggestions.push(
                    OptimizationSuggestion::new(
                        OptimizationCategory::TokenOptimization,
                        vec![exec.node.clone()],
                        format!(
                            "Node '{}' uses {:.1}% of total tokens ({} tokens)",
                            exec.node, token_percentage, exec.tokens_used
                        ),
                        "Reduce token usage to lower costs and improve response times".to_string(),
                        format!(
                            "Consider: 1) Summarizing conversation history, 2) Reducing system prompt size, 3) Using more efficient prompt templates for '{}'",
                            exec.node
                        ),
                    )
                    .with_priority(OptimizationPriority::High)
                    .with_effort(3)
                    .with_confidence(0.7)
                    .with_related_bottleneck(BottleneckMetric::TokenUsage)
                    .with_evidence(format!(
                        "Uses {} tokens vs average {:.0} tokens",
                        exec.tokens_used, avg_tokens
                    )),
                );
            } else if token_percentage > 30.0 && exec.tokens_used as f64 > avg_tokens * 2.0 {
                analysis.suggestions.push(
                    OptimizationSuggestion::new(
                        OptimizationCategory::TokenOptimization,
                        vec![exec.node.clone()],
                        format!(
                            "Node '{}' uses significantly more tokens than average ({} vs avg {:.0})",
                            exec.node, exec.tokens_used, avg_tokens
                        ),
                        "Consider reviewing prompt efficiency".to_string(),
                        format!(
                            "Review the prompt template for '{}' - it may contain unnecessary context or verbose instructions.",
                            exec.node
                        ),
                    )
                    .with_priority(OptimizationPriority::Medium)
                    .with_effort(2)
                    .with_confidence(0.5)
                    .with_related_bottleneck(BottleneckMetric::TokenUsage),
                );
            }
        }
    }

    fn detect_error_handling_opportunities(
        &self,
        analysis: &mut OptimizationAnalysis,
        health_deductions: &mut f64,
    ) {
        // Find nodes with failed executions
        let mut node_errors: HashMap<&str, (usize, usize)> = HashMap::new(); // (errors, total)

        for exec in &self.nodes_executed {
            let entry = node_errors.entry(&exec.node).or_default();
            entry.1 += 1;
            if !exec.success {
                entry.0 += 1;
            }
        }

        for (node, (errors, total)) in &node_errors {
            if *errors > 0 {
                let error_rate = *errors as f64 / *total as f64;

                let (priority, effort) = if error_rate >= 0.5 {
                    *health_deductions += 0.15;
                    (OptimizationPriority::Critical, 4)
                } else if error_rate >= 0.25 {
                    *health_deductions += 0.1;
                    (OptimizationPriority::High, 3)
                } else if error_rate >= 0.1 {
                    *health_deductions += 0.05;
                    (OptimizationPriority::Medium, 3)
                } else {
                    (OptimizationPriority::Low, 2)
                };

                analysis.suggestions.push(
                    OptimizationSuggestion::new(
                        OptimizationCategory::ErrorHandling,
                        vec![node.to_string()],
                        format!(
                            "Node '{}' has {:.1}% error rate ({}/{} executions failed)",
                            node,
                            error_rate * 100.0,
                            errors,
                            total
                        ),
                        "Improve reliability and reduce failed executions".to_string(),
                        format!(
                            "Add retry logic with exponential backoff for '{}'. Consider: 1) Input validation, 2) Fallback responses, 3) Circuit breaker pattern for external calls.",
                            node
                        ),
                    )
                    .with_priority(priority)
                    .with_effort(effort)
                    .with_confidence(0.8)
                    .with_related_bottleneck(BottleneckMetric::ErrorRate)
                    .with_evidence(format!(
                        "{} errors out of {} executions",
                        errors, total
                    )),
                );
            }
        }
    }

    fn detect_frequency_reduction_opportunities(
        &self,
        analysis: &mut OptimizationAnalysis,
        health_deductions: &mut f64,
    ) {
        // Find nodes called excessively
        let mut node_counts: HashMap<&str, usize> = HashMap::new();
        for exec in &self.nodes_executed {
            *node_counts.entry(&exec.node).or_default() += 1;
        }

        for (node, count) in &node_counts {
            if *count >= 20 {
                let priority = if *count >= 100 {
                    *health_deductions += 0.15;
                    OptimizationPriority::Critical
                } else if *count >= 50 {
                    *health_deductions += 0.1;
                    OptimizationPriority::High
                } else {
                    *health_deductions += 0.05;
                    OptimizationPriority::Medium
                };

                analysis.suggestions.push(
                    OptimizationSuggestion::new(
                        OptimizationCategory::FrequencyReduction,
                        vec![node.to_string()],
                        format!("Node '{}' executed {} times", node, count),
                        "Reduce execution frequency to improve overall performance".to_string(),
                        format!(
                            "Review the execution logic for '{}'. Consider: 1) Adding termination conditions, 2) Batching operations, 3) Adding maximum iteration limits.",
                            node
                        ),
                    )
                    .with_priority(priority)
                    .with_effort(3)
                    .with_confidence(0.7)
                    .with_related_bottleneck(BottleneckMetric::HighFrequency)
                    .with_evidence(format!(
                        "{} executions in a single trace",
                        count
                    )),
                );
            }
        }
    }

    fn detect_stabilization_opportunities(
        &self,
        analysis: &mut OptimizationAnalysis,
        health_deductions: &mut f64,
    ) {
        // Find nodes with high variance in execution time
        let mut node_durations: HashMap<&str, Vec<u64>> = HashMap::new();
        for exec in &self.nodes_executed {
            node_durations
                .entry(&exec.node)
                .or_default()
                .push(exec.duration_ms);
        }

        for (node, durations) in &node_durations {
            if durations.len() >= 3 {
                let mean = durations.iter().sum::<u64>() as f64 / durations.len() as f64;
                if mean > 0.0 {
                    let variance = durations
                        .iter()
                        .map(|d| (*d as f64 - mean).powi(2))
                        .sum::<f64>()
                        / durations.len() as f64;
                    let std_dev = variance.sqrt();
                    let cv = std_dev / mean;

                    if cv > 1.0 {
                        let priority = if cv >= 2.0 {
                            *health_deductions += 0.08;
                            OptimizationPriority::High
                        } else {
                            *health_deductions += 0.04;
                            OptimizationPriority::Medium
                        };

                        analysis.suggestions.push(
                            OptimizationSuggestion::new(
                                OptimizationCategory::Stabilization,
                                vec![node.to_string()],
                                format!(
                                    "Node '{}' has unpredictable execution times (CV: {:.2})",
                                    node, cv
                                ),
                                "Improve execution time consistency for better predictability".to_string(),
                                format!(
                                    "Investigate what causes variance in '{}'. Consider: 1) Adding timeouts, 2) Normalizing inputs, 3) Checking external dependency stability.",
                                    node
                                ),
                            )
                            .with_priority(priority)
                            .with_effort(4)
                            .with_confidence(0.6)
                            .with_related_bottleneck(BottleneckMetric::HighVariance)
                            .with_evidence(format!(
                                "Mean: {:.0} ms, Std Dev: {:.0} ms, CV: {:.2}",
                                mean, std_dev, cv
                            )),
                        );
                    }
                }
            }
        }
    }
}

// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    // =========================================================================
    // OptimizationCategory Tests
    // =========================================================================

    #[test]
    fn test_optimization_category_display() {
        assert_eq!(OptimizationCategory::Caching.to_string(), "caching");
        assert_eq!(
            OptimizationCategory::Parallelization.to_string(),
            "parallelization"
        );
        assert_eq!(
            OptimizationCategory::ModelChoice.to_string(),
            "model_choice"
        );
        assert_eq!(
            OptimizationCategory::TokenOptimization.to_string(),
            "token_optimization"
        );
        assert_eq!(
            OptimizationCategory::ErrorHandling.to_string(),
            "error_handling"
        );
        assert_eq!(
            OptimizationCategory::FrequencyReduction.to_string(),
            "frequency_reduction"
        );
        assert_eq!(
            OptimizationCategory::Stabilization.to_string(),
            "stabilization"
        );
        assert_eq!(OptimizationCategory::Performance.to_string(), "performance");
    }

    #[test]
    fn test_optimization_category_equality() {
        assert_eq!(OptimizationCategory::Caching, OptimizationCategory::Caching);
        assert_ne!(
            OptimizationCategory::Caching,
            OptimizationCategory::Parallelization
        );
    }

    #[test]
    fn test_optimization_category_clone() {
        let cat = OptimizationCategory::ModelChoice;
        let cloned = cat.clone();
        assert_eq!(cat, cloned);
    }

    #[test]
    fn test_optimization_category_debug() {
        let debug = format!("{:?}", OptimizationCategory::TokenOptimization);
        assert!(debug.contains("TokenOptimization"));
    }

    #[test]
    fn test_optimization_category_serialization() {
        let cat = OptimizationCategory::Caching;
        let json = serde_json::to_string(&cat).unwrap();
        let deserialized: OptimizationCategory = serde_json::from_str(&json).unwrap();
        assert_eq!(cat, deserialized);
    }

    // =========================================================================
    // OptimizationPriority Tests
    // =========================================================================

    #[test]
    fn test_optimization_priority_display() {
        assert_eq!(OptimizationPriority::Low.to_string(), "low");
        assert_eq!(OptimizationPriority::Medium.to_string(), "medium");
        assert_eq!(OptimizationPriority::High.to_string(), "high");
        assert_eq!(OptimizationPriority::Critical.to_string(), "critical");
    }

    #[test]
    fn test_optimization_priority_default() {
        let default = OptimizationPriority::default();
        assert_eq!(default, OptimizationPriority::Low);
    }

    #[test]
    fn test_optimization_priority_ordering() {
        assert!(OptimizationPriority::Low < OptimizationPriority::Medium);
        assert!(OptimizationPriority::Medium < OptimizationPriority::High);
        assert!(OptimizationPriority::High < OptimizationPriority::Critical);
    }

    #[test]
    #[allow(clippy::clone_on_copy)]
    fn test_optimization_priority_clone_copy() {
        let p = OptimizationPriority::High;
        let cloned = p.clone();
        let copied = p; // Copy
        assert_eq!(p, cloned);
        assert_eq!(p, copied);
    }

    #[test]
    fn test_optimization_priority_serialization() {
        let priority = OptimizationPriority::Critical;
        let json = serde_json::to_string(&priority).unwrap();
        let deserialized: OptimizationPriority = serde_json::from_str(&json).unwrap();
        assert_eq!(priority, deserialized);
    }

    // =========================================================================
    // OptimizationSuggestion Tests
    // =========================================================================

    #[test]
    fn test_suggestion_new() {
        let suggestion = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["node1".to_string()],
            "Cache results",
            "Save 50ms",
            "Add cache layer",
        );

        assert_eq!(suggestion.category, OptimizationCategory::Caching);
        assert_eq!(suggestion.target_nodes, vec!["node1".to_string()]);
        assert_eq!(suggestion.description, "Cache results");
        assert_eq!(suggestion.expected_improvement, "Save 50ms");
        assert_eq!(suggestion.implementation, "Add cache layer");
        assert_eq!(suggestion.priority, OptimizationPriority::Medium); // Default
        assert_eq!(suggestion.effort, 3); // Default
        assert!((suggestion.confidence - 0.5).abs() < f64::EPSILON); // Default
        assert!(suggestion.related_bottleneck.is_none());
        assert!(suggestion.evidence.is_empty());
    }

    #[test]
    fn test_suggestion_with_methods() {
        let suggestion = OptimizationSuggestion::new(
            OptimizationCategory::Parallelization,
            vec!["node_a".to_string(), "node_b".to_string()],
            "Run in parallel",
            "40% faster",
            "Use parallel edges",
        )
        .with_priority(OptimizationPriority::High)
        .with_effort(2)
        .with_confidence(0.8)
        .with_related_bottleneck(BottleneckMetric::Latency)
        .with_evidence("Evidence 1")
        .with_evidence("Evidence 2");

        assert_eq!(suggestion.priority, OptimizationPriority::High);
        assert_eq!(suggestion.effort, 2);
        assert!((suggestion.confidence - 0.8).abs() < f64::EPSILON);
        assert_eq!(
            suggestion.related_bottleneck,
            Some(BottleneckMetric::Latency)
        );
        assert_eq!(suggestion.evidence.len(), 2);
    }

    #[test]
    fn test_suggestion_effort_clamping() {
        let suggestion_low = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_effort(0);
        assert_eq!(suggestion_low.effort, 1);

        let suggestion_high = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_effort(10);
        assert_eq!(suggestion_high.effort, 5);
    }

    #[test]
    fn test_suggestion_confidence_clamping() {
        let suggestion_low = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_confidence(-0.5);
        assert!((suggestion_low.confidence - 0.0).abs() < f64::EPSILON);

        let suggestion_high = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_confidence(1.5);
        assert!((suggestion_high.confidence - 1.0).abs() < f64::EPSILON);
    }

    #[test]
    fn test_suggestion_is_high_priority() {
        let low = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_priority(OptimizationPriority::Low);
        assert!(!low.is_high_priority());

        let medium = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_priority(OptimizationPriority::Medium);
        assert!(!medium.is_high_priority());

        let high = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_priority(OptimizationPriority::High);
        assert!(high.is_high_priority());

        let critical = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_priority(OptimizationPriority::Critical);
        assert!(critical.is_high_priority());
    }

    #[test]
    fn test_suggestion_is_low_effort() {
        let low_effort = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_effort(1);
        assert!(low_effort.is_low_effort());

        let low_effort_2 = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_effort(2);
        assert!(low_effort_2.is_low_effort());

        let med_effort = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_effort(3);
        assert!(!med_effort.is_low_effort());
    }

    #[test]
    fn test_suggestion_quick_win_score() {
        // High priority, low effort, high confidence = best quick win
        let best = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_priority(OptimizationPriority::Critical) // 4.0
        .with_effort(1) // effort_score = 5.0
        .with_confidence(1.0);
        // Score = (4.0 * 5.0 * 1.0) / 4.0 = 5.0
        assert!((best.quick_win_score() - 5.0).abs() < f64::EPSILON);

        // Low priority, high effort, low confidence = worst quick win
        let worst = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        )
        .with_priority(OptimizationPriority::Low) // 1.0
        .with_effort(5) // effort_score = 1.0
        .with_confidence(0.1);
        // Score = (1.0 * 1.0 * 0.1) / 4.0 = 0.025
        assert!((worst.quick_win_score() - 0.025).abs() < f64::EPSILON);
    }

    #[test]
    fn test_suggestion_summary() {
        let suggestion = OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["node1".to_string(), "node2".to_string()],
            "Cache these nodes",
            "50% reduction",
            "Add cache",
        )
        .with_priority(OptimizationPriority::High);

        let summary = suggestion.summary();
        assert!(summary.contains("[high]"));
        assert!(summary.contains("caching"));
        assert!(summary.contains("node1, node2"));
        assert!(summary.contains("Cache these nodes"));
        assert!(summary.contains("50% reduction"));
    }

    #[test]
    fn test_suggestion_json_roundtrip() {
        let suggestion = OptimizationSuggestion::new(
            OptimizationCategory::Parallelization,
            vec!["async_node".to_string()],
            "Make parallel",
            "2x faster",
            "Use tokio::spawn",
        )
        .with_priority(OptimizationPriority::Medium)
        .with_effort(3)
        .with_confidence(0.75)
        .with_evidence("Test evidence");

        let json = suggestion.to_json().unwrap();
        let parsed = OptimizationSuggestion::from_json(&json).unwrap();

        assert_eq!(parsed.category, suggestion.category);
        assert_eq!(parsed.target_nodes, suggestion.target_nodes);
        assert_eq!(parsed.description, suggestion.description);
        assert_eq!(parsed.priority, suggestion.priority);
    }

    // =========================================================================
    // OptimizationSuggestionBuilder Tests
    // =========================================================================

    #[test]
    fn test_builder_new() {
        let builder = OptimizationSuggestionBuilder::new();
        // Defaults: effort=3, confidence=0.5
        let result = builder
            .category(OptimizationCategory::Caching)
            .target_node("test_node")
            .description("Test description")
            .expected_improvement("Test improvement")
            .implementation("Test impl")
            .build();

        let suggestion = result.unwrap();
        assert_eq!(suggestion.effort, 3);
        assert!((suggestion.confidence - 0.5).abs() < f64::EPSILON);
    }

    #[test]
    fn test_builder_all_fields() {
        let suggestion = OptimizationSuggestion::builder()
            .category(OptimizationCategory::TokenOptimization)
            .target_node("llm_node")
            .target_node("summarizer")
            .description("Reduce token usage")
            .expected_improvement("30% cost reduction")
            .implementation("Compress prompts")
            .priority(OptimizationPriority::High)
            .effort(2)
            .confidence(0.9)
            .related_bottleneck(BottleneckMetric::TokenUsage)
            .evidence("Evidence line 1")
            .evidence("Evidence line 2")
            .build()
            .unwrap();

        assert_eq!(suggestion.category, OptimizationCategory::TokenOptimization);
        assert_eq!(suggestion.target_nodes.len(), 2);
        assert!(suggestion.target_nodes.contains(&"llm_node".to_string()));
        assert!(suggestion.target_nodes.contains(&"summarizer".to_string()));
        assert_eq!(suggestion.priority, OptimizationPriority::High);
        assert_eq!(suggestion.effort, 2);
        assert!((suggestion.confidence - 0.9).abs() < f64::EPSILON);
        assert_eq!(
            suggestion.related_bottleneck,
            Some(BottleneckMetric::TokenUsage)
        );
        assert_eq!(suggestion.evidence.len(), 2);
    }

    #[test]
    fn test_builder_target_nodes_vector() {
        let suggestion = OptimizationSuggestion::builder()
            .category(OptimizationCategory::Caching)
            .target_nodes(vec!["a".to_string(), "b".to_string(), "c".to_string()])
            .description("d")
            .expected_improvement("e")
            .implementation("i")
            .build()
            .unwrap();

        assert_eq!(suggestion.target_nodes.len(), 3);
    }

    #[test]
    fn test_builder_error_missing_category() {
        let result = OptimizationSuggestion::builder()
            .target_node("node")
            .description("d")
            .expected_improvement("e")
            .implementation("i")
            .build();

        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "category is required");
    }

    #[test]
    fn test_builder_error_missing_description() {
        let result = OptimizationSuggestion::builder()
            .category(OptimizationCategory::Caching)
            .target_node("node")
            .expected_improvement("e")
            .implementation("i")
            .build();

        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "description is required");
    }

    #[test]
    fn test_builder_error_missing_expected_improvement() {
        let result = OptimizationSuggestion::builder()
            .category(OptimizationCategory::Caching)
            .target_node("node")
            .description("d")
            .implementation("i")
            .build();

        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "expected_improvement is required");
    }

    #[test]
    fn test_builder_error_missing_implementation() {
        let result = OptimizationSuggestion::builder()
            .category(OptimizationCategory::Caching)
            .target_node("node")
            .description("d")
            .expected_improvement("e")
            .build();

        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "implementation is required");
    }

    #[test]
    fn test_builder_error_no_target_nodes() {
        let result = OptimizationSuggestion::builder()
            .category(OptimizationCategory::Caching)
            .description("d")
            .expected_improvement("e")
            .implementation("i")
            .build();

        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "at least one target node is required");
    }

    #[test]
    fn test_builder_effort_clamping() {
        let suggestion = OptimizationSuggestion::builder()
            .category(OptimizationCategory::Caching)
            .target_node("n")
            .description("d")
            .expected_improvement("e")
            .implementation("i")
            .effort(100)
            .build()
            .unwrap();

        assert_eq!(suggestion.effort, 5);
    }

    #[test]
    fn test_builder_confidence_clamping() {
        let suggestion = OptimizationSuggestion::builder()
            .category(OptimizationCategory::Caching)
            .target_node("n")
            .description("d")
            .expected_improvement("e")
            .implementation("i")
            .confidence(5.0)
            .build()
            .unwrap();

        assert!((suggestion.confidence - 1.0).abs() < f64::EPSILON);
    }

    // =========================================================================
    // OptimizationAnalysis Tests
    // =========================================================================

    #[test]
    fn test_analysis_new() {
        let analysis = OptimizationAnalysis::new();
        assert!(analysis.suggestions.is_empty());
        assert_eq!(analysis.patterns_analyzed, 0);
        assert!((analysis.health_score - 1.0).abs() < f64::EPSILON);
        assert!(analysis.summary.is_empty());
    }

    #[test]
    fn test_analysis_default() {
        let analysis = OptimizationAnalysis::default();
        assert!(!analysis.has_suggestions());
        assert_eq!(analysis.suggestion_count(), 0);
    }

    #[test]
    fn test_analysis_has_suggestions() {
        let mut analysis = OptimizationAnalysis::new();
        assert!(!analysis.has_suggestions());

        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n".to_string()],
            "d",
            "e",
            "i",
        ));
        assert!(analysis.has_suggestions());
    }

    #[test]
    fn test_analysis_by_category() {
        let mut analysis = OptimizationAnalysis::new();
        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n1".to_string()],
            "d1",
            "e1",
            "i1",
        ));
        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Parallelization,
            vec!["n2".to_string()],
            "d2",
            "e2",
            "i2",
        ));
        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["n3".to_string()],
            "d3",
            "e3",
            "i3",
        ));

        let caching = analysis.by_category(&OptimizationCategory::Caching);
        assert_eq!(caching.len(), 2);

        let parallel = analysis.by_category(&OptimizationCategory::Parallelization);
        assert_eq!(parallel.len(), 1);

        let model = analysis.by_category(&OptimizationCategory::ModelChoice);
        assert!(model.is_empty());
    }

    #[test]
    fn test_analysis_by_priority() {
        let mut analysis = OptimizationAnalysis::new();
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n1".to_string()],
                "d1",
                "e1",
                "i1",
            )
            .with_priority(OptimizationPriority::High),
        );
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n2".to_string()],
                "d2",
                "e2",
                "i2",
            )
            .with_priority(OptimizationPriority::Low),
        );
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n3".to_string()],
                "d3",
                "e3",
                "i3",
            )
            .with_priority(OptimizationPriority::High),
        );

        let high = analysis.by_priority(OptimizationPriority::High);
        assert_eq!(high.len(), 2);

        let low = analysis.by_priority(OptimizationPriority::Low);
        assert_eq!(low.len(), 1);
    }

    #[test]
    fn test_analysis_high_priority() {
        let mut analysis = OptimizationAnalysis::new();
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n".to_string()],
                "d",
                "e",
                "i",
            )
            .with_priority(OptimizationPriority::Low),
        );
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n".to_string()],
                "d",
                "e",
                "i",
            )
            .with_priority(OptimizationPriority::Medium),
        );
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n".to_string()],
                "d",
                "e",
                "i",
            )
            .with_priority(OptimizationPriority::High),
        );
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n".to_string()],
                "d",
                "e",
                "i",
            )
            .with_priority(OptimizationPriority::Critical),
        );

        let high_priority = analysis.high_priority();
        assert_eq!(high_priority.len(), 2); // High and Critical
    }

    #[test]
    fn test_analysis_for_node() {
        let mut analysis = OptimizationAnalysis::new();
        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["nodeA".to_string(), "nodeB".to_string()],
            "d1",
            "e1",
            "i1",
        ));
        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["nodeB".to_string()],
            "d2",
            "e2",
            "i2",
        ));
        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["nodeC".to_string()],
            "d3",
            "e3",
            "i3",
        ));

        let for_a = analysis.for_node("nodeA");
        assert_eq!(for_a.len(), 1);

        let for_b = analysis.for_node("nodeB");
        assert_eq!(for_b.len(), 2);

        let for_d = analysis.for_node("nodeD");
        assert!(for_d.is_empty());
    }

    #[test]
    fn test_analysis_quick_wins() {
        let mut analysis = OptimizationAnalysis::new();

        // Add multiple suggestions with varying quick win scores
        for i in 0..10 {
            analysis.suggestions.push(
                OptimizationSuggestion::new(
                    OptimizationCategory::Caching,
                    vec![format!("node{}", i)],
                    format!("desc{}", i),
                    "e",
                    "i",
                )
                .with_priority(if i < 3 {
                    OptimizationPriority::Critical
                } else {
                    OptimizationPriority::Low
                })
                .with_effort(if i < 3 { 1 } else { 5 })
                .with_confidence(0.8),
            );
        }

        let quick_wins = analysis.quick_wins();
        assert_eq!(quick_wins.len(), 5); // Returns top 5
                                         // First ones should be high priority, low effort
        assert_eq!(quick_wins[0].priority, OptimizationPriority::Critical);
    }

    #[test]
    fn test_analysis_generate_summary_no_suggestions() {
        let analysis = OptimizationAnalysis::new();
        let summary = analysis.generate_summary();
        assert!(summary.contains("No optimization suggestions"));
    }

    #[test]
    fn test_analysis_generate_summary_with_suggestions() {
        let mut analysis = OptimizationAnalysis::new();
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n".to_string()],
                "d",
                "e",
                "i",
            )
            .with_priority(OptimizationPriority::High),
        );
        analysis.suggestions.push(
            OptimizationSuggestion::new(
                OptimizationCategory::Caching,
                vec!["n".to_string()],
                "d",
                "e",
                "i",
            )
            .with_priority(OptimizationPriority::Medium),
        );
        analysis.health_score = 0.85;

        let summary = analysis.generate_summary();
        assert!(summary.contains("Found 2 optimization suggestions"));
        assert!(summary.contains("high priority"));
        assert!(summary.contains("medium priority"));
        assert!(summary.contains("85%"));
    }

    #[test]
    fn test_analysis_json_roundtrip() {
        let mut analysis = OptimizationAnalysis::new();
        analysis.suggestions.push(OptimizationSuggestion::new(
            OptimizationCategory::Caching,
            vec!["node".to_string()],
            "desc",
            "improve",
            "impl",
        ));
        analysis.patterns_analyzed = 5;
        analysis.health_score = 0.9;
        analysis.summary = "Test summary".to_string();

        let json = analysis.to_json().unwrap();
        let parsed = OptimizationAnalysis::from_json(&json).unwrap();

        assert_eq!(parsed.suggestion_count(), 1);
        assert_eq!(parsed.patterns_analyzed, 5);
        assert!((parsed.health_score - 0.9).abs() < f64::EPSILON);
    }

    // =========================================================================
    // ExecutionTrace::suggest_optimizations Tests
    // =========================================================================

    fn make_node_execution(
        node: &str,
        duration_ms: u64,
        tokens: u64,
        success: bool,
    ) -> NodeExecution {
        NodeExecution {
            node: node.to_string(),
            duration_ms,
            tokens_used: tokens,
            success,
            state_before: None,
            state_after: None,
            tools_called: Vec::new(),
            error_message: None,
            index: 0,
            started_at: None,
            metadata: HashMap::new(),
        }
    }

    fn make_test_trace(nodes: Vec<NodeExecution>) -> ExecutionTrace {
        let total_duration: u64 = nodes.iter().map(|n| n.duration_ms).sum();
        let total_tokens: u64 = nodes.iter().map(|n| n.tokens_used).sum();
        ExecutionTrace {
            thread_id: None,
            execution_id: Some("test".to_string()),
            parent_execution_id: None,
            root_execution_id: None,
            depth: Some(0),
            nodes_executed: nodes,
            total_duration_ms: total_duration,
            total_tokens,
            errors: Vec::new(),
            completed: true,
            started_at: None,
            ended_at: None,
            final_state: None,
            metadata: HashMap::new(),
            execution_metrics: None,
            performance_metrics: None,
        }
    }

    #[test]
    fn test_suggest_optimizations_empty_trace() {
        let trace = make_test_trace(vec![]);

        let analysis = trace.suggest_optimizations();
        assert!(!analysis.has_suggestions());
        assert_eq!(analysis.patterns_analyzed, 7);
        assert!((analysis.health_score - 1.0).abs() < f64::EPSILON);
    }

    #[test]
    fn test_suggest_optimizations_caching_opportunity() {
        // Node executed 5+ times with consistent timing = caching candidate
        let trace = make_test_trace(
            (0..6)
                .map(|_| make_node_execution("cache_candidate", 100, 50, true))
                .collect(),
        );

        let analysis = trace.suggest_optimizations();
        let caching = analysis.by_category(&OptimizationCategory::Caching);
        assert!(!caching.is_empty());
        assert!(caching[0].description.contains("cache_candidate"));
    }

    #[test]
    fn test_suggest_optimizations_high_frequency_node() {
        // Node executed 50+ times = frequency reduction suggestion
        let trace = make_test_trace(
            (0..50)
                .map(|_| make_node_execution("frequent_node", 10, 5, true))
                .collect(),
        );

        let analysis = trace.suggest_optimizations();
        let freq = analysis.by_category(&OptimizationCategory::FrequencyReduction);
        assert!(!freq.is_empty());
        assert!(freq[0].description.contains("50 times"));
    }

    #[test]
    fn test_suggest_optimizations_error_handling() {
        // Node with high error rate = error handling suggestion
        let nodes: Vec<NodeExecution> = (0..10)
            .map(|i| make_node_execution("flaky_node", 100, 50, i < 3)) // 30% success
            .collect();

        let trace = make_test_trace(nodes);

        let analysis = trace.suggest_optimizations();
        let errors = analysis.by_category(&OptimizationCategory::ErrorHandling);
        assert!(!errors.is_empty());
        assert!(errors[0].description.contains("error rate"));
    }

    #[test]
    fn test_suggest_optimizations_token_heavy_node() {
        // Single node using >50% of total tokens = token optimization suggestion
        let trace = make_test_trace(vec![
            make_node_execution("token_heavy", 500, 8000, true),
            make_node_execution("light_node", 100, 1000, true),
            make_node_execution("other_node", 100, 1000, true),
        ]);

        let analysis = trace.suggest_optimizations();
        let tokens = analysis.by_category(&OptimizationCategory::TokenOptimization);
        assert!(!tokens.is_empty());
        assert!(tokens[0].target_nodes.contains(&"token_heavy".to_string()));
    }

    #[test]
    fn test_suggest_optimizations_model_choice() {
        // High tokens but fast execution = potentially over-provisioned model
        let trace = make_test_trace(vec![make_node_execution("fast_llm", 100, 2000, true)]);

        let analysis = trace.suggest_optimizations();
        let model = analysis.by_category(&OptimizationCategory::ModelChoice);
        assert!(!model.is_empty());
    }

    #[test]
    fn test_suggest_optimizations_stabilization() {
        // High variance in execution time = stabilization suggestion
        // Stabilization requires CV > 1.0, so we need extreme variance
        // Times: 50, 1000, 50, 1500, 50 -> mean=530, CVâ‰ˆ1.15
        let trace = make_test_trace(vec![
            make_node_execution("unstable", 50, 100, true),
            make_node_execution("unstable", 1000, 100, true),
            make_node_execution("unstable", 50, 100, true),
            make_node_execution("unstable", 1500, 100, true),
            make_node_execution("unstable", 50, 100, true),
        ]);

        let analysis = trace.suggest_optimizations();
        let stable = analysis.by_category(&OptimizationCategory::Stabilization);
        assert!(!stable.is_empty());
        assert!(stable[0].description.contains("unpredictable"));
    }

    #[test]
    fn test_suggest_optimizations_parallelization() {
        // Multiple sequential nodes with similar duration = potential parallelization
        let trace = make_test_trace(vec![
            make_node_execution("node_a", 200, 50, true),
            make_node_execution("node_b", 180, 50, true),
            make_node_execution("node_c", 220, 50, true),
            make_node_execution("node_d", 190, 50, true),
        ]);

        let analysis = trace.suggest_optimizations();
        let parallel = analysis.by_category(&OptimizationCategory::Parallelization);
        // May or may not detect depending on heuristics, just check it runs
        assert!(analysis.patterns_analyzed == 7);
        // The parallelization detection depends on savings threshold (>100ms)
        // With 4 nodes at ~200ms each, max_time is ~220ms, total is ~790ms
        // Potential savings = 790 - 220 = 570ms, which exceeds threshold
        assert!(!parallel.is_empty() || analysis.health_score <= 1.0);
    }

    #[test]
    fn test_suggest_optimizations_health_score_degradation() {
        // Multiple issues should degrade health score
        let mut nodes: Vec<NodeExecution> = (0..60)
            .map(|_| make_node_execution("frequent", 10, 5, true))
            .collect();
        // Add error-prone node
        for i in 0..10 {
            nodes.push(make_node_execution("flaky", 100, 50, i < 5)); // 50% error rate
        }

        let trace = make_test_trace(nodes);

        let analysis = trace.suggest_optimizations();
        assert!(analysis.health_score < 1.0);
        assert!(analysis.has_suggestions());
    }

    #[test]
    fn test_suggest_optimizations_suggestions_sorted_by_priority() {
        // Create trace that generates multiple suggestions of different priorities
        let mut nodes: Vec<NodeExecution> = (0..10)
            .map(|i| make_node_execution("critical_flaky", 100, 50, i < 4))
            .collect();
        // Add other nodes
        nodes.push(make_node_execution("other", 100, 50, true));

        let trace = make_test_trace(nodes);

        let analysis = trace.suggest_optimizations();
        // Verify suggestions are sorted by priority (highest first)
        let suggestions = &analysis.suggestions;
        for i in 1..suggestions.len() {
            assert!(suggestions[i - 1].priority >= suggestions[i].priority);
        }
    }

    #[test]
    fn test_suggest_optimizations_no_token_opt_with_zero_total() {
        // Edge case: total_tokens = 0 should not cause divide by zero
        let trace = make_test_trace(vec![make_node_execution("node", 100, 0, true)]);

        let analysis = trace.suggest_optimizations();
        // Should complete without panic
        assert_eq!(analysis.patterns_analyzed, 7);
    }

    #[test]
    fn test_suggest_optimizations_caching_with_high_variance_skipped() {
        // Node executed multiple times but with HIGH variance = NOT a caching candidate
        let trace = make_test_trace(vec![
            make_node_execution("variable_node", 10, 50, true),
            make_node_execution("variable_node", 500, 50, true),
            make_node_execution("variable_node", 20, 50, true),
            make_node_execution("variable_node", 1000, 50, true),
            make_node_execution("variable_node", 30, 50, true),
        ]);

        let analysis = trace.suggest_optimizations();
        let caching = analysis.by_category(&OptimizationCategory::Caching);
        // High variance (CV > 0.5) means caching not suggested
        assert!(caching.is_empty());
    }

    #[test]
    fn test_suggest_optimizations_critical_error_rate() {
        // Very high error rate should produce Critical priority
        let nodes: Vec<NodeExecution> = (0..20)
            .map(|i| make_node_execution("failing_node", 100, 50, i < 2)) // 90% failure rate
            .collect();

        let trace = make_test_trace(nodes);

        let analysis = trace.suggest_optimizations();
        let errors = analysis.by_category(&OptimizationCategory::ErrorHandling);
        assert!(!errors.is_empty());
        assert_eq!(errors[0].priority, OptimizationPriority::Critical);
    }

    #[test]
    fn test_suggest_optimizations_multiple_categories() {
        // Trace with multiple issues across categories
        let trace = make_test_trace(vec![
            // High token usage
            make_node_execution("token_hog", 500, 6000, true),
            // Fast but token-heavy (model choice)
            make_node_execution("fast_expensive", 50, 1500, true),
            make_node_execution("other", 100, 500, true),
        ]);

        let analysis = trace.suggest_optimizations();
        // Should detect token optimization
        let tokens = analysis.by_category(&OptimizationCategory::TokenOptimization);
        assert!(!tokens.is_empty());
        // Should also detect model choice
        let model = analysis.by_category(&OptimizationCategory::ModelChoice);
        assert!(!model.is_empty());
    }

    #[test]
    fn test_suggest_optimizations_caching_with_10_executions() {
        // 10+ executions = high priority caching
        let trace = make_test_trace(
            (0..12)
                .map(|_| make_node_execution("repeated", 100, 50, true))
                .collect(),
        );

        let analysis = trace.suggest_optimizations();
        let caching = analysis.by_category(&OptimizationCategory::Caching);
        assert!(!caching.is_empty());
        assert_eq!(caching[0].priority, OptimizationPriority::High);
    }

    #[test]
    fn test_suggest_optimizations_100_executions() {
        // 100+ executions = critical frequency reduction
        let trace = make_test_trace(
            (0..105)
                .map(|_| make_node_execution("loop_node", 5, 2, true))
                .collect(),
        );

        let analysis = trace.suggest_optimizations();
        let freq = analysis.by_category(&OptimizationCategory::FrequencyReduction);
        assert!(!freq.is_empty());
        assert_eq!(freq[0].priority, OptimizationPriority::Critical);
    }
}
